{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099a8a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. The summer we turned sixteen felt endless, filled with laughter and late-night adventures\n",
      "2. Every afternoon we rode our bikes to the lake, racing each other down the dusty road until the sun dipped low\n",
      "3. Evenings were spent around a small bonfire, telling stories, roasting marshmallows, and making promises we thought would last forever\n",
      "4. The air was warm, the stars seemed brighter, and for the first time we felt like the world belonged to us\n",
      "5. Those days were simple, but they carried the kind of magic that only youth and friendship can create\n",
      "\n",
      "Words: ['the', 'summer', 'we', 'turned', 'sixteen', 'felt', 'endless', 'filled', 'with', 'laughter', 'and', 'late', 'night', 'adventures', 'every', 'afternoon', 'we', 'rode', 'our', 'bikes', 'to', 'the', 'lake', 'racing', 'each', 'other', 'down', 'the', 'dusty', 'road', 'until', 'the', 'sun', 'dipped', 'low', 'evenings', 'were', 'spent', 'around', 'a', 'small', 'bonfire', 'telling', 'stories', 'roasting', 'marshmallows', 'and', 'making', 'promises', 'we', 'thought', 'would', 'last', 'forever', 'the', 'air', 'was', 'warm', 'the', 'stars', 'seemed', 'brighter', 'and', 'for', 'the', 'first', 'time', 'we', 'felt', 'like', 'the', 'world', 'belonged', 'to', 'us', 'those', 'days', 'were', 'simple', 'but', 'they', 'carried', 'the', 'kind', 'of', 'magic', 'that', 'only', 'youth', 'and', 'friendship', 'can', 'create']\n",
      "Total words: 93\n",
      "Unique words: 76\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def analyze_paragraph(paragraph):\n",
    "    # Split into sentences using regex (., ?, ! as delimiters)\n",
    "    sentences = re.split(r'[.!?]\\s*', paragraph.strip())\n",
    "    sentences = [s for s in sentences if s]  # remove empty strings\n",
    "    \n",
    "    # Split into words (alphanumeric only)\n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "    \n",
    "    # Count total words\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Count unique words\n",
    "    unique_words = set(words)\n",
    "    unique_count = len(unique_words)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Sentences:\")\n",
    "    for i, s in enumerate(sentences, 1):\n",
    "        print(f\"{i}. {s}\")\n",
    "    \n",
    "    print(\"\\nWords:\", words)\n",
    "    print(\"Total words:\", total_words)\n",
    "    print(\"Unique words:\", unique_count)\n",
    "\n",
    "# Example usage\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "analyze_paragraph(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae744e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. The summer we turned sixteen felt endless, filled with laughter and late-night adventures\n",
      "2. Every afternoon we rode our bikes to the lake, racing each other down the dusty road until the sun dipped low\n",
      "3. Evenings were spent around a small bonfire, telling stories, roasting marshmallows, and making promises we thought would last forever\n",
      "4. The air was warm, the stars seemed brighter, and for the first time we felt like the world belonged to us\n",
      "5. Those days were simple, but they carried the kind of magic that only youth and friendship can create\n",
      "\n",
      "Words (without stop words): ['summer', 'we', 'turned', 'sixteen', 'felt', 'endless', 'filled', 'laughter', 'late', 'night', 'adventures', 'every', 'afternoon', 'we', 'rode', 'our', 'bikes', 'lake', 'racing', 'each', 'other', 'down', 'dusty', 'road', 'until', 'sun', 'dipped', 'low', 'evenings', 'spent', 'around', 'small', 'bonfire', 'telling', 'stories', 'roasting', 'marshmallows', 'making', 'promises', 'we', 'thought', 'would', 'last', 'forever', 'air', 'warm', 'stars', 'seemed', 'brighter', 'first', 'time', 'we', 'felt', 'like', 'world', 'belonged', 'us', 'days', 'simple', 'but', 'they', 'carried', 'kind', 'magic', 'only', 'youth', 'friendship', 'can', 'create']\n",
      "Total words (without stop words): 69\n",
      "Unique words: 65\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a simple list of stop words\n",
    "stop_words = {\n",
    "    \"the\", \"is\", \"are\", \"and\", \"a\", \"an\", \"to\", \"of\", \"in\", \"on\", \"for\", \"with\",\n",
    "    \"at\", \"by\", \"from\", \"up\", \"about\", \"into\", \"over\", \"after\", \"it\", \"this\",\n",
    "    \"that\", \"these\", \"those\", \"be\", \"was\", \"were\", \"been\", \"am\", \"do\", \"does\",\n",
    "    \"did\", \"so\", \"such\", \"as\"\n",
    "}\n",
    "\n",
    "def analyze_paragraph(paragraph):\n",
    "    # Split into sentences using regex\n",
    "    sentences = re.split(r'[.!?]\\s*', paragraph.strip())\n",
    "    sentences = [s for s in sentences if s]  # remove empty strings\n",
    "    \n",
    "    # Extract words (lowercase, alphanumeric only)\n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    # Count total and unique words\n",
    "    total_words = len(filtered_words)\n",
    "    unique_words = set(filtered_words)\n",
    "    unique_count = len(unique_words)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Sentences:\")\n",
    "    for i, s in enumerate(sentences, 1):\n",
    "        print(f\"{i}. {s}\")\n",
    "    \n",
    "    print(\"\\nWords (without stop words):\", filtered_words)\n",
    "    print(\"Total words (without stop words):\", total_words)\n",
    "    print(\"Unique words:\", unique_count)\n",
    "\n",
    "# Example usage\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "analyze_paragraph(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d18c7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['every', 'summer', 'felt', 'like', 'a', 'new', 'beginning', 'the', 'ocean', 'whispered', 'secrets', 'that', 'only', 'the', 'waves', 'could', 'keep', 'and', 'the', 'air', 'carried', 'the', 'promise', 'of', 'long', 'nights', 'filled', 'with', 'laughter', 'belly', 'believed', 'that', 'summers', 'were', 'more', 'than', 'just', 'seasons', 'they', 'were', 'chapters', 'of', 'her', 'life', 'written', 'in', 'sand', 'and', 'remembered', 'in', 'saltwater', 'friends', 'came', 'and', 'went', 'but', 'the', 'memories', 'stayed', 'glowing', 'brighter', 'each', 'year', 'like', 'fireflies', 'in', 'the', 'dark']\n",
      "Stemmed Words: ['everi', 'summer', 'felt', 'like', 'a', 'new', 'begin', 'the', 'ocean', 'whisper', 'secret', 'that', 'onli', 'the', 'wave', 'could', 'keep', 'and', 'the', 'air', 'carri', 'the', 'promis', 'of', 'long', 'night', 'fill', 'with', 'laughter', 'belli', 'believ', 'that', 'summer', 'were', 'more', 'than', 'just', 'season', 'they', 'were', 'chapter', 'of', 'her', 'life', 'written', 'in', 'sand', 'and', 'rememb', 'in', 'saltwat', 'friend', 'came', 'and', 'went', 'but', 'the', 'memori', 'stay', 'glow', 'brighter', 'each', 'year', 'like', 'firefli', 'in', 'the', 'dark']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_paragraph(paragraph):\n",
    "    # Initialize stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Extract words (lowercase, alphanumeric only)\n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "    \n",
    "    # Apply stemming\n",
    "    stems = [stemmer.stem(w) for w in words]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Original Words:\", words)\n",
    "    print(\"Stemmed Words:\", stems)\n",
    "\n",
    "# Example usage\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "stem_paragraph(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746c9153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maheit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maheit\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['the', 'summer', 'we', 'turned', 'sixteen', 'felt', 'endless', 'filled', 'with', 'laughter', 'and', 'late', 'night', 'adventures', 'every', 'afternoon', 'we', 'rode', 'our', 'bikes', 'to', 'the', 'lake', 'racing', 'each', 'other', 'down', 'the', 'dusty', 'road', 'until', 'the', 'sun', 'dipped', 'low', 'evenings', 'were', 'spent', 'around', 'a', 'small', 'bonfire', 'telling', 'stories', 'roasting', 'marshmallows', 'and', 'making', 'promises', 'we', 'thought', 'would', 'last', 'forever', 'the', 'air', 'was', 'warm', 'the', 'stars', 'seemed', 'brighter', 'and', 'for', 'the', 'first', 'time', 'we', 'felt', 'like', 'the', 'world', 'belonged', 'to', 'us', 'those', 'days', 'were', 'simple', 'but', 'they', 'carried', 'the', 'kind', 'of', 'magic', 'that', 'only', 'youth', 'and', 'friendship', 'can', 'create']\n",
      "Lemmatized Words: ['the', 'summer', 'we', 'turned', 'sixteen', 'felt', 'endless', 'filled', 'with', 'laughter', 'and', 'late', 'night', 'adventure', 'every', 'afternoon', 'we', 'rode', 'our', 'bike', 'to', 'the', 'lake', 'racing', 'each', 'other', 'down', 'the', 'dusty', 'road', 'until', 'the', 'sun', 'dipped', 'low', 'evening', 'were', 'spent', 'around', 'a', 'small', 'bonfire', 'telling', 'story', 'roasting', 'marshmallow', 'and', 'making', 'promise', 'we', 'thought', 'would', 'last', 'forever', 'the', 'air', 'wa', 'warm', 'the', 'star', 'seemed', 'brighter', 'and', 'for', 'the', 'first', 'time', 'we', 'felt', 'like', 'the', 'world', 'belonged', 'to', 'u', 'those', 'day', 'were', 'simple', 'but', 'they', 'carried', 'the', 'kind', 'of', 'magic', 'that', 'only', 'youth', 'and', 'friendship', 'can', 'create']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data (only needed once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_paragraph(paragraph):\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Extract words (lowercase, alphanumeric only)\n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "    \n",
    "    # Apply lemmatization (default noun form)\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Original Words:\", words)\n",
    "    print(\"Lemmatized Words:\", lemmas)\n",
    "\n",
    "# Example usage\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "lemmatize_paragraph(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a95cb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\maheit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the correct POS tagger resource\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae458ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['The', 'summer', 'we', 'turned', 'sixteen', 'felt', 'endless', ',', 'filled', 'with', 'laughter', 'and', 'late', '-', 'night', 'adventures', '.', 'Every', 'afternoon', 'we', 'rode', 'our', 'bikes', 'to', 'the', 'lake', ',', 'racing', 'each', 'other', 'down', 'the', 'dusty', 'road', 'until', 'the', 'sun', 'dipped', 'low', '.', 'Evenings', 'were', 'spent', 'around', 'a', 'small', 'bonfire', ',', 'telling', 'stories', ',', 'roasting', 'marshmallows', ',', 'and', 'making', 'promises', 'we', 'thought', 'would', 'last', 'forever', '.', 'The', 'air', 'was', 'warm', ',', 'the', 'stars', 'seemed', 'brighter', ',', 'and', 'for', 'the', 'first', 'time', 'we', 'felt', 'like', 'the', 'world', 'belonged', 'to', 'us', '.', 'Those', 'days', 'were', 'simple', ',', 'but', 'they', 'carried', 'the', 'kind', 'of', 'magic', 'that', 'only', 'youth', 'and', 'friendship', 'can', 'create', '.']\n",
      "Lemmatized Words: ['the', 'summer', 'we', 'turn', 'sixteen', 'feel', 'endless', ',', 'fill', 'with', 'laughter', 'and', 'late', '-', 'night', 'adventure', '.', 'every', 'afternoon', 'we', 'ride', 'our', 'bike', 'to', 'the', 'lake', ',', 'race', 'each', 'other', 'down', 'the', 'dusty', 'road', 'until', 'the', 'sun', 'dip', 'low', '.', 'evening', 'be', 'spend', 'around', 'a', 'small', 'bonfire', ',', 'tell', 'story', ',', 'roast', 'marshmallow', ',', 'and', 'make', 'promise', 'we', 'think', 'would', 'last', 'forever', '.', 'the', 'air', 'be', 'warm', ',', 'the', 'star', 'seem', 'bright', ',', 'and', 'for', 'the', 'first', 'time', 'we', 'feel', 'like', 'the', 'world', 'belong', 'to', 'we', '.', 'those', 'day', 'be', 'simple', ',', 'but', 'they', 'carry', 'the', 'kind', 'of', 'magic', 'that', 'only', 'youth', 'and', 'friendship', 'can', 'create', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model (install once: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "doc = nlp(paragraph)\n",
    "\n",
    "print(\"Original Words:\", [token.text for token in doc])\n",
    "print(\"Lemmatized Words:\", [token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb1e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequencies:\n",
      "the: 9\n",
      "summer: 1\n",
      "we: 4\n",
      "turned: 1\n",
      "sixteen: 1\n",
      "felt: 2\n",
      "endless: 1\n",
      "filled: 1\n",
      "with: 1\n",
      "laughter: 1\n",
      "and: 4\n",
      "late: 1\n",
      "night: 1\n",
      "adventures: 1\n",
      "every: 1\n",
      "afternoon: 1\n",
      "rode: 1\n",
      "our: 1\n",
      "bikes: 1\n",
      "to: 2\n",
      "lake: 1\n",
      "racing: 1\n",
      "each: 1\n",
      "other: 1\n",
      "down: 1\n",
      "dusty: 1\n",
      "road: 1\n",
      "until: 1\n",
      "sun: 1\n",
      "dipped: 1\n",
      "low: 1\n",
      "evenings: 1\n",
      "were: 2\n",
      "spent: 1\n",
      "around: 1\n",
      "a: 1\n",
      "small: 1\n",
      "bonfire: 1\n",
      "telling: 1\n",
      "stories: 1\n",
      "roasting: 1\n",
      "marshmallows: 1\n",
      "making: 1\n",
      "promises: 1\n",
      "thought: 1\n",
      "would: 1\n",
      "last: 1\n",
      "forever: 1\n",
      "air: 1\n",
      "was: 1\n",
      "warm: 1\n",
      "stars: 1\n",
      "seemed: 1\n",
      "brighter: 1\n",
      "for: 1\n",
      "first: 1\n",
      "time: 1\n",
      "like: 1\n",
      "world: 1\n",
      "belonged: 1\n",
      "us: 1\n",
      "those: 1\n",
      "days: 1\n",
      "simple: 1\n",
      "but: 1\n",
      "they: 1\n",
      "carried: 1\n",
      "kind: 1\n",
      "of: 1\n",
      "magic: 1\n",
      "that: 1\n",
      "only: 1\n",
      "youth: 1\n",
      "friendship: 1\n",
      "can: 1\n",
      "create: 1\n",
      "\n",
      "Top 5 Most Frequent Words:\n",
      "the: 9\n",
      "we: 4\n",
      "and: 4\n",
      "felt: 2\n",
      "to: 2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def word_frequency(paragraph):\n",
    "    # Convert to lowercase and extract words (ignore punctuation)\n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    freq = Counter(words)\n",
    "    \n",
    "    # Print all word frequencies\n",
    "    print(\"Word Frequencies:\")\n",
    "    for word, count in freq.items():\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    # Print top 5 most frequent words\n",
    "    print(\"\\nTop 5 Most Frequent Words:\")\n",
    "    for word, count in freq.most_common(5):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "# Example usage\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "word_frequency(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e1889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['the', 'summer', 'we', 'turned', 'sixteen', 'felt', 'endless', 'filled', 'with', 'laughter', 'and', 'late', 'night', 'adventures', 'every', 'afternoon', 'we', 'rode', 'our', 'bikes', 'to', 'the', 'lake', 'racing', 'each', 'other', 'down', 'the', 'dusty', 'road', 'until', 'the', 'sun', 'dipped', 'low', 'evenings', 'were', 'spent', 'around', 'a', 'small', 'bonfire', 'telling', 'stories', 'roasting', 'marshmallows', 'and', 'making', 'promises', 'we', 'thought', 'would', 'last', 'forever', 'the', 'air', 'was', 'warm', 'the', 'stars', 'seemed', 'brighter', 'and', 'for', 'the', 'first', 'time', 'we', 'felt', 'like', 'the', 'world', 'belonged', 'to', 'us', 'those', 'days', 'were', 'simple', 'but', 'they', 'carried', 'the', 'kind', 'of', 'magic', 'that', 'only', 'youth', 'and', 'friendship', 'can', 'create']\n",
      "\n",
      "Bigrams:\n",
      "('the', 'summer')\n",
      "('summer', 'we')\n",
      "('we', 'turned')\n",
      "('turned', 'sixteen')\n",
      "('sixteen', 'felt')\n",
      "('felt', 'endless')\n",
      "('endless', 'filled')\n",
      "('filled', 'with')\n",
      "('with', 'laughter')\n",
      "('laughter', 'and')\n",
      "('and', 'late')\n",
      "('late', 'night')\n",
      "('night', 'adventures')\n",
      "('adventures', 'every')\n",
      "('every', 'afternoon')\n",
      "('afternoon', 'we')\n",
      "('we', 'rode')\n",
      "('rode', 'our')\n",
      "('our', 'bikes')\n",
      "('bikes', 'to')\n",
      "('to', 'the')\n",
      "('the', 'lake')\n",
      "('lake', 'racing')\n",
      "('racing', 'each')\n",
      "('each', 'other')\n",
      "('other', 'down')\n",
      "('down', 'the')\n",
      "('the', 'dusty')\n",
      "('dusty', 'road')\n",
      "('road', 'until')\n",
      "('until', 'the')\n",
      "('the', 'sun')\n",
      "('sun', 'dipped')\n",
      "('dipped', 'low')\n",
      "('low', 'evenings')\n",
      "('evenings', 'were')\n",
      "('were', 'spent')\n",
      "('spent', 'around')\n",
      "('around', 'a')\n",
      "('a', 'small')\n",
      "('small', 'bonfire')\n",
      "('bonfire', 'telling')\n",
      "('telling', 'stories')\n",
      "('stories', 'roasting')\n",
      "('roasting', 'marshmallows')\n",
      "('marshmallows', 'and')\n",
      "('and', 'making')\n",
      "('making', 'promises')\n",
      "('promises', 'we')\n",
      "('we', 'thought')\n",
      "('thought', 'would')\n",
      "('would', 'last')\n",
      "('last', 'forever')\n",
      "('forever', 'the')\n",
      "('the', 'air')\n",
      "('air', 'was')\n",
      "('was', 'warm')\n",
      "('warm', 'the')\n",
      "('the', 'stars')\n",
      "('stars', 'seemed')\n",
      "('seemed', 'brighter')\n",
      "('brighter', 'and')\n",
      "('and', 'for')\n",
      "('for', 'the')\n",
      "('the', 'first')\n",
      "('first', 'time')\n",
      "('time', 'we')\n",
      "('we', 'felt')\n",
      "('felt', 'like')\n",
      "('like', 'the')\n",
      "('the', 'world')\n",
      "('world', 'belonged')\n",
      "('belonged', 'to')\n",
      "('to', 'us')\n",
      "('us', 'those')\n",
      "('those', 'days')\n",
      "('days', 'were')\n",
      "('were', 'simple')\n",
      "('simple', 'but')\n",
      "('but', 'they')\n",
      "('they', 'carried')\n",
      "('carried', 'the')\n",
      "('the', 'kind')\n",
      "('kind', 'of')\n",
      "('of', 'magic')\n",
      "('magic', 'that')\n",
      "('that', 'only')\n",
      "('only', 'youth')\n",
      "('youth', 'and')\n",
      "('and', 'friendship')\n",
      "('friendship', 'can')\n",
      "('can', 'create')\n",
      "\n",
      "Trigrams:\n",
      "('the', 'summer', 'we')\n",
      "('summer', 'we', 'turned')\n",
      "('we', 'turned', 'sixteen')\n",
      "('turned', 'sixteen', 'felt')\n",
      "('sixteen', 'felt', 'endless')\n",
      "('felt', 'endless', 'filled')\n",
      "('endless', 'filled', 'with')\n",
      "('filled', 'with', 'laughter')\n",
      "('with', 'laughter', 'and')\n",
      "('laughter', 'and', 'late')\n",
      "('and', 'late', 'night')\n",
      "('late', 'night', 'adventures')\n",
      "('night', 'adventures', 'every')\n",
      "('adventures', 'every', 'afternoon')\n",
      "('every', 'afternoon', 'we')\n",
      "('afternoon', 'we', 'rode')\n",
      "('we', 'rode', 'our')\n",
      "('rode', 'our', 'bikes')\n",
      "('our', 'bikes', 'to')\n",
      "('bikes', 'to', 'the')\n",
      "('to', 'the', 'lake')\n",
      "('the', 'lake', 'racing')\n",
      "('lake', 'racing', 'each')\n",
      "('racing', 'each', 'other')\n",
      "('each', 'other', 'down')\n",
      "('other', 'down', 'the')\n",
      "('down', 'the', 'dusty')\n",
      "('the', 'dusty', 'road')\n",
      "('dusty', 'road', 'until')\n",
      "('road', 'until', 'the')\n",
      "('until', 'the', 'sun')\n",
      "('the', 'sun', 'dipped')\n",
      "('sun', 'dipped', 'low')\n",
      "('dipped', 'low', 'evenings')\n",
      "('low', 'evenings', 'were')\n",
      "('evenings', 'were', 'spent')\n",
      "('were', 'spent', 'around')\n",
      "('spent', 'around', 'a')\n",
      "('around', 'a', 'small')\n",
      "('a', 'small', 'bonfire')\n",
      "('small', 'bonfire', 'telling')\n",
      "('bonfire', 'telling', 'stories')\n",
      "('telling', 'stories', 'roasting')\n",
      "('stories', 'roasting', 'marshmallows')\n",
      "('roasting', 'marshmallows', 'and')\n",
      "('marshmallows', 'and', 'making')\n",
      "('and', 'making', 'promises')\n",
      "('making', 'promises', 'we')\n",
      "('promises', 'we', 'thought')\n",
      "('we', 'thought', 'would')\n",
      "('thought', 'would', 'last')\n",
      "('would', 'last', 'forever')\n",
      "('last', 'forever', 'the')\n",
      "('forever', 'the', 'air')\n",
      "('the', 'air', 'was')\n",
      "('air', 'was', 'warm')\n",
      "('was', 'warm', 'the')\n",
      "('warm', 'the', 'stars')\n",
      "('the', 'stars', 'seemed')\n",
      "('stars', 'seemed', 'brighter')\n",
      "('seemed', 'brighter', 'and')\n",
      "('brighter', 'and', 'for')\n",
      "('and', 'for', 'the')\n",
      "('for', 'the', 'first')\n",
      "('the', 'first', 'time')\n",
      "('first', 'time', 'we')\n",
      "('time', 'we', 'felt')\n",
      "('we', 'felt', 'like')\n",
      "('felt', 'like', 'the')\n",
      "('like', 'the', 'world')\n",
      "('the', 'world', 'belonged')\n",
      "('world', 'belonged', 'to')\n",
      "('belonged', 'to', 'us')\n",
      "('to', 'us', 'those')\n",
      "('us', 'those', 'days')\n",
      "('those', 'days', 'were')\n",
      "('days', 'were', 'simple')\n",
      "('were', 'simple', 'but')\n",
      "('simple', 'but', 'they')\n",
      "('but', 'they', 'carried')\n",
      "('they', 'carried', 'the')\n",
      "('carried', 'the', 'kind')\n",
      "('the', 'kind', 'of')\n",
      "('kind', 'of', 'magic')\n",
      "('of', 'magic', 'that')\n",
      "('magic', 'that', 'only')\n",
      "('that', 'only', 'youth')\n",
      "('only', 'youth', 'and')\n",
      "('youth', 'and', 'friendship')\n",
      "('and', 'friendship', 'can')\n",
      "('friendship', 'can', 'create')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def generate_ngrams(paragraph):\n",
    "    # Extract words (lowercase, alphanumeric only)\n",
    "    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n",
    "    \n",
    "    # Generate bigrams (2-word sequences)\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    \n",
    "    # Generate trigrams (3-word sequences)\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Words:\", words)\n",
    "    print(\"\\nBigrams:\")\n",
    "    for bg in bigrams:\n",
    "        print(bg)\n",
    "    \n",
    "    print(\"\\nTrigrams:\")\n",
    "    for tg in trigrams:\n",
    "        print(tg)\n",
    "\n",
    "# Example usage\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "generate_ngrams(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85857b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The → DET\n",
      "summer → NOUN\n",
      "we → PRON\n",
      "turned → VERB\n",
      "sixteen → NUM\n",
      "felt → VERB\n",
      "endless → ADJ\n",
      ", → PUNCT\n",
      "filled → VERB\n",
      "with → ADP\n",
      "laughter → NOUN\n",
      "and → CCONJ\n",
      "late → ADJ\n",
      "- → PUNCT\n",
      "night → NOUN\n",
      "adventures → NOUN\n",
      ". → PUNCT\n",
      "Every → DET\n",
      "afternoon → NOUN\n",
      "we → PRON\n",
      "rode → VERB\n",
      "our → PRON\n",
      "bikes → NOUN\n",
      "to → ADP\n",
      "the → DET\n",
      "lake → NOUN\n",
      ", → PUNCT\n",
      "racing → VERB\n",
      "each → DET\n",
      "other → ADJ\n",
      "down → ADP\n",
      "the → DET\n",
      "dusty → ADJ\n",
      "road → NOUN\n",
      "until → SCONJ\n",
      "the → DET\n",
      "sun → NOUN\n",
      "dipped → VERB\n",
      "low → ADV\n",
      ". → PUNCT\n",
      "Evenings → NOUN\n",
      "were → AUX\n",
      "spent → VERB\n",
      "around → ADP\n",
      "a → DET\n",
      "small → ADJ\n",
      "bonfire → NOUN\n",
      ", → PUNCT\n",
      "telling → VERB\n",
      "stories → NOUN\n",
      ", → PUNCT\n",
      "roasting → VERB\n",
      "marshmallows → NOUN\n",
      ", → PUNCT\n",
      "and → CCONJ\n",
      "making → VERB\n",
      "promises → NOUN\n",
      "we → PRON\n",
      "thought → VERB\n",
      "would → AUX\n",
      "last → VERB\n",
      "forever → ADV\n",
      ". → PUNCT\n",
      "The → DET\n",
      "air → NOUN\n",
      "was → AUX\n",
      "warm → ADJ\n",
      ", → PUNCT\n",
      "the → DET\n",
      "stars → NOUN\n",
      "seemed → VERB\n",
      "brighter → ADJ\n",
      ", → PUNCT\n",
      "and → CCONJ\n",
      "for → ADP\n",
      "the → DET\n",
      "first → ADJ\n",
      "time → NOUN\n",
      "we → PRON\n",
      "felt → VERB\n",
      "like → SCONJ\n",
      "the → DET\n",
      "world → NOUN\n",
      "belonged → VERB\n",
      "to → ADP\n",
      "us → PRON\n",
      ". → PUNCT\n",
      "Those → DET\n",
      "days → NOUN\n",
      "were → AUX\n",
      "simple → ADJ\n",
      ", → PUNCT\n",
      "but → CCONJ\n",
      "they → PRON\n",
      "carried → VERB\n",
      "the → DET\n",
      "kind → NOUN\n",
      "of → ADP\n",
      "magic → NOUN\n",
      "that → SCONJ\n",
      "only → ADV\n",
      "youth → NOUN\n",
      "and → CCONJ\n",
      "friendship → NOUN\n",
      "can → AUX\n",
      "create → VERB\n",
      ". → PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_tag_sentence_spacy(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Example usage\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "tags = pos_tag_sentence_spacy(sentence)\n",
    "for word, pos in tags:\n",
    "    print(f\"{word} → {pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698564c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
